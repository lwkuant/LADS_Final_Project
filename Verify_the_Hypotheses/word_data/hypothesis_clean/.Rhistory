pred
}
for(v in catVars) {
pi <- paste('pred',v,sep='')
dTrain[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dTrain[,v])
dCal[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dCal[,v])
dTest[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dTest[,v])
}
# Scoring categorical variables by AUC
library('ROCR')
calcAUC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'auc')
as.numeric(perf@y.values)
}
for(v in catVars) {
pi <- paste('pred',v,sep='')
aucTrain <- calcAUC(dTrain[,pi],dTrain[,outcome])
if(aucTrain>=0.8) {
aucCal <- calcAUC(dCal[,pi],dCal[,outcome])
print(sprintf("%s, trainAUC: %4.3f calibrationAUC: %4.3f",
pi,aucTrain,aucCal))
}
}
# Scoring numeric variables by AUC
mkPredN <- function(outCol,varCol,appCol) {
cuts <- unique(as.numeric(quantile(varCol,
probs=seq(0, 1, 0.1),na.rm=T)))
varC <- cut(varCol,cuts)
appC <- cut(appCol,cuts)
mkPredC(outCol,varC,appC)
}
for(v in numericVars) {
pi <- paste('pred',v,sep='')
dTrain[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dTrain[,v])
dTest[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dTest[,v])
dCal[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dCal[,v])
aucTrain <- calcAUC(dTrain[,pi],dTrain[,outcome])
if(aucTrain>=0.55) {
aucCal <- calcAUC(dCal[,pi],dCal[,outcome])
print(sprintf("%s, trainAUC: %4.3f calibrationAUC: %4.3f",
pi,aucTrain,aucCal))
}
}
library(ggplot2)
ggplot(data=dCal) +
geom_density(aes(x=predVar126,color=as.factor(churn)))
# Running a repeated cross-validation experiment
var <- 'Var217'
aucs <- rep(0,100)
for(rep in 1:length(aucs)) {
useForCalRep <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0
predRep <- mkPredC(dTrainAll[!useForCalRep,outcome],
dTrainAll[!useForCalRep,var],
dTrainAll[useForCalRep,var])
aucs[rep] <- calcAUC(predRep,dTrainAll[useForCalRep,outcome])
}
mean(aucs)
sd(aucs)
# Empirically cross-validating performance
fCross <- function() {
useForCalRep <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0
predRep <- mkPredC(dTrainAll[!useForCalRep,outcome],
dTrainAll[!useForCalRep,var],dTrainAll[useForCalRep,var])
calcAUC(predRep,dTrainAll[useForCalRep,outcome])
}
aucs <- replicate(100,fCross())
aucs
## Basic variable selection
logLikelyhood <-
function(outCol,predCol) {
sum(ifelse(outCol==pos,log(predCol),log(1-predCol)))
}
selVars <- c()
minStep <- 5
baseRateCheck <- logLikelyhood(dCal[,outcome],
sum(dCal[,outcome]==pos)/length(dCal[,outcome]))
for(v in catVars) {
pi <- paste('pred',v,sep='')
liCheck <- 2*((logLikelyhood(dCal[,outcome],dCal[,pi]) -
baseRateCheck))
if(liCheck>minStep) {
print(sprintf("%s, calibrationScore: %g",
pi,liCheck))
selVars <- c(selVars,pi)
}
}
for(v in numericVars) {
pi <- paste('pred',v,sep='')
liCheck <- 2*((logLikelyhood(dCal[,outcome],dCal[,pi]) -
baseRateCheck) - 1)
if(liCheck>=minStep) {
print(sprintf("%s, calibrationScore: %g",
pi,liCheck))
selVars <- c(selVars,pi)
}
}
# Building a bad decision tree
library('rpart')
fV <- paste(outcome,'>0 ~ ',
paste(c(catVars,numericVars),collapse=' + '),sep='')
tmodel <- rpart(fV,data=dTrain)
print(calcAUC(predict(tmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(tmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(tmodel,newdata=dCal),dCal[,outcome]))
# Building another bad decision tree
tVars <- paste('pred',c(catVars,numericVars),sep='')
fV2 <- paste(outcome,'>0 ~ ',paste(tVars,collapse=' + '),sep='')
tmodel <- rpart(fV2,data=dTrain)
print(calcAUC(predict(tmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(tmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(tmodel,newdata=dCal),dCal[,outcome]))
# Building yet another bad decision tree
tmodel <- rpart(fV2,data=dTrain,
control=rpart.control(cp=0.001,minsplit=1000,
minbucket=1000,maxdepth=5)
)
print(calcAUC(predict(tmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(tmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(tmodel,newdata=dCal),dCal[,outcome]))
# Building a better decision tree
f <- paste(outcome,'>0 ~ ',paste(selVars,collapse=' + '),sep='')
tmodel <- rpart(f,data=dTrain,
control=rpart.control(cp=0.001,minsplit=1000,
minbucket=1000,maxdepth=5)
)
print(calcAUC(predict(tmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(tmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(tmodel,newdata=dCal),dCal[,outcome]))
print(tmodel)
tmodel
par(cex=0.7)
plot(tmodel)
text(tmodel)
print(plotROC(knnPred(dTest[,selVars]),dTest[,outcome]))
plotROC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'tpr','fpr')
pf <- data.frame(
FalsePositiveRate=perf@x.values[[1]],
TruePositiveRate=perf@y.values[[1]])
ggplot() +
geom_line(data=pf,aes(x=FalsePositiveRate,y=TruePositiveRate)) +
geom_line(aes(x=c(0,1),y=c(0,1)))
}
print(plotROC(knnPred(dTest[,selVars]),dTest[,outcome]))
library('class')
nK <- 200
knnTrain <- dTrain[,selVars]
knnCl <- dTrain[,outcome]==pos
knnPred <- function(df) {
knnDecision <- knn(knnTrain,df,knnCl,k=nK,prob=T)
ifelse(knnDecision==TRUE,
attributes(knnDecision)$prob,
1-(attributes(knnDecision)$prob))
}
print(calcAUC(knnPred(dTrain[,selVars]),dTrain[,outcome]))
print(calcAUC(knnPred(dCal[,selVars]),dCal[,outcome]))
print(calcAUC(knnPred(dTest[,selVars]),dTest[,outcome]))
dCal$kpred <- knnPred(dCal[,selVars])
ggplot(data=dCal) +
geom_density(aes(x=kpred,
color=as.factor(churn),linetype=as.factor(churn)))
plotROC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'tpr','fpr')
pf <- data.frame(
FalsePositiveRate=perf@x.values[[1]],
TruePositiveRate=perf@y.values[[1]])
ggplot() +
geom_line(data=pf,aes(x=FalsePositiveRate,y=TruePositiveRate)) +
geom_line(aes(x=c(0,1),y=c(0,1)))
}
print(plotROC(knnPred(dTest[,selVars]),dTest[,outcome]))
gmodel <- glm(as.formula(f),data=dTrain,family=binomial(link='logit'))
print(calcAUC(predict(gmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(gmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(gmodel,newdata=dCal),dCal[,outcome]))
gmodel <- glm(as.formula(f),data=dTrain,family=binomial(link='logit'))
print(calcAUC(predict(gmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(gmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(gmodel,newdata=dCal),dCal[,outcome]))
# VIDEO 4
# Read in data
wine = read.csv("D:/Kile/Coding/R programming/MITx 15.071x The Analytics Edge/Unit 2/wine.csv")
str(wine)
summary(wine)
# Linear Regression (one variable)
model1 = lm(Price ~ AGST, data=wine)
summary(model1)
# Sum of Squared Errors
model1$residuals
SSE = sum(model1$residuals^2)
SSE
# Linear Regression (two variables)
model2 = lm(Price ~ AGST + HarvestRain, data=wine)
summary(model2)
# Sum of Squared Errors
SSE = sum(model2$residuals^2)
SSE
# Linear Regression (all variables)
model3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
# Sum of Squared Errors
SSE = sum(model3$residuals^2)
SSE
# VIDEO 5
# Remove FrancePop
model4 = lm(Price ~ AGST + HarvestRain + WinterRain + Age, data=wine)
summary(model4)
model3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
# Sum of Squared Errors
SSE = sum(model3$residuals^2)
SSE
# VIDEO 5
# Remove FrancePop
model4 = lm(Price ~ AGST + HarvestRain + WinterRain + Age, data=wine)
summary(model4)
# VIDEO 6
# Correlations
cor(wine$WinterRain, wine$Price)
cor(wine$Age, wine$FrancePop)
cor(wine)
# Remove Age and FrancePop
model5 = lm(Price ~ AGST + HarvestRain + WinterRain, data=wine)
summary(model5)
# VIDEO 7
# Read in test set
wineTest = read.csv("D:/Kile/Coding/R programming/MITx 15.071x The Analytics Edge/Unit 2/wine_test.csv")
str(wineTest)
# Make test set predictions
predictTest = predict(model4, newdata=wineTest)
predictTest
# Compute R-squared
SSE = sum((wineTest$Price - predictTest)^2)
SST = sum((wineTest$Price - mean(wine$Price))^2)
1 - SSE/SST
# Unit 6 - Introduction to Clustering
# Video 6
# After following the steps in the video, load the data into R
movies = read.table("D:/Kile/Coding/R programming/MITx 15.071x The Analytics Edge/Unit 6/movieLens.txt", header=FALSE, sep="|",quote="\"")
View(movies)
?re.table
?read.table
str(movies)
# Add column names
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
str(movies)
# Remove unnecessary variables
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
# Remove duplicates
movies = unique(movies)
# Take a look at our data again:
str(movies)
# Video 7
# Compute distances
distances = dist(movies[2:20], method = "euclidean")
# Hierarchical clustering
clusterMovies = hclust(distances, method = "ward.D")
# Plot the dendrogram
plot(clusterMovies)
# Assign points to clusters
# k is equal to the number of clusters
clusterGroups = cutree(clusterMovies, k = 10)
#Now let's figure out what the clusters are like.
# Let's use the tapply function to compute the percentage of movies in each genre and cluster
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
clusterGroups
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
# We can repeat this for each genre. If you do, you get the results in ClusterMeans.ods
# Find which cluster Men in Black is in.
subset(movies, Title=="Men in Black (1997)")
clusterGroups[257]
# Create a new data set with just the movies from cluster 2
cluster2 = subset(movies, clusterGroups==2)
# Look at the first 10 titles in this cluster:
cluster2$Title[1:10]
clusterGroups[257]
# Create a new data set with just the movies from cluster 2
cluster2 = subset(movies, clusterGroups==2)
# Look at the first 10 titles in this cluster:
cluster2$Title[1:10]
View(baseball)
library(jiebaR)
a = "我是，好孩子，YA哈哈! no~ 笨笨!"
a
cutter = worker()
word = cutter[a]
word
class(word)
table(word)
paste(word[1], word[2])
paste(word[1], word[2],sep = "")
b = c("我", "是", "哈哈")
b
for(i in 1:length(word)){}
TRUE & TRUE
word_new = c()
for(i in 1:length(word)){
if(word[i] %in% b & word[i+1] %in% b){
word_new = c(word_new, paste(word[i], word[i+1], sep = ""))
}
}
word_new
seq(1,10,0.5)
length(seq(1,10,0.5))
length(seq(1,2,0.5))
x = scan()
x
chickwts
feeds = table(chickwts$feed)
feeds
barplot(feeds[order(feeds, decreasing = T)])
rep(c(1,2), each = 10)
summary(cars)
summary(iris)
str(cars)
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("V1", substr(list.files()[1],1,nchar(list.files()[1])-12))
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp,by = "V1", all.x = T)
}
df[is.na(df)] = 0
mean_words = sapply(2:(ncol(df)), function(i) mean(df[,i]))
cv_words = sapply(2:(ncol(df)), function(i) sd(df[,i])/mean(df[,i]))
cv_words
sapply(2:(ncol(df)), function(i) mean(df[,i]))
df$time_ind = rep(0, nrow(df))
View(df)
View(df)
colnames(df)[1] = "Date"
View(df)
df$time_ind = rep(0, nrow(df))
for(i in 1:nrow(df)){
df$time_ind[i] = substr(df$Date[i],6,7)
}
tapply(df[,2:32], df$time_ind, sum)
tapply(df[,2], df$time_ind, sum)
tapply(df[,3], df$time_ind, sum)
tapply(df[,4], df$time_ind, sum)
df_month = data.frame(a = rep(0,12))
View(df_month)
df_month = data.frame(a = rep(0,12))
for(i in 2:32){
df_month$colnames(df)[i] = tapply(df[,i], df$time_ind, sum)
}
tapply(df[,2], df$time_ind, sum)
class(tapply(df[,2], df$time_ind, sum))
tapply(df[,2], df$time_ind, sum)[1]
tapply(df[,2], df$time_ind, sum)[2]
as.vector(tapply(df[,2], df$time_ind, sum))
df_month = data.frame(a = rep(0,12))
for(i in 2:32){
df_month$colnames(df)[i] = as.vector(tapply(df[,i], df$time_ind, sum))
}
df_month$colnames(df)[2] = as.vector(tapply(df[,2], df$time_ind, sum))
colnames(df)[2]
for(i in 2:32){
df_month$(colnames(df)[i]) = as.vector(tapply(df[,i], df$time_ind, sum))
}
df_month = data.frame(a = rep(0,12))
for(i in 2:32){
df_month$(colnames(df)[i]) = as.vector(tapply(df[,i], df$time_ind, sum))
}
df_month$colnames(df)[2] = rep(0,12)
df_month$ddd = rep(0,12)
df_month = data.frame(a = rep(0,12))
colnames(df)[2]
as.formula(colnames(df)[2])
df_month = data.frame(a = rep(0,12))
for(i in 2:32){
df_month = cbind(df_month, as.vector(tapply(df[,i], df$time_ind, sum)))
colnames(df_month)[i] = colnames(df)[i]
}
colnames(df_month)[1] = "Month"
df_month[1] = 1:12
mean_words = sapply(2:(ncol(df_month)), function(i) mean(df_month[,i]))
cv_words = sapply(2:(ncol(df_month)), function(i) sd(df_month[,i])/mean(df_month[,i]))
mean_words
cv_words
setwd("D:/Kile/語言分析與資料科學/Final/word_data/hypothesis_clean")
setwd("D:/Kile/語言分析與資料科學/Final/word_data/hypothesis_clean")
metaphorlity = read.csv("metaphorlity.csv", stringsAsFactors = F)
View(metaphorlity)
sentility = read.csv("sentility.csv", stringsAsFactors = F)
sour = read.csv("sour.csv", stringsAsFactors = F)
View(sour)
View(df)
colnames(df)[2]
colnames(df)[3]
colnames(df)[4]
colnames(df)[4] = "Z>B"
colnames(df_month)[4] = "Z>B"
View(df)
View(sentility)
metaphorlity[,1] = NULL
colnames(metaphorlity)[1:3]
colnames(metaphorlity)[1:3] = c("2.0", "Z>B", "over my dead body")
sentility[,1] = NULL
colnames(sentility)[1:3] = c("2.0", "Z>B", "over my dead body")
sour[,1] = NULL
colnames(sour)[1:3] = c("2.0", "Z>B", "over my dead body")
t(sour)
t = t(sour)
View(t)
match(colnames(df),colnames(sour))
a = c(1,2,3)
b = c(3,1,2)
match(a,b)
a[match(a,b)]
b[match(a,b)]
match(colnames(df)[2:32],colnames(sour))
name_ind = match(colnames(df)[2:32],colnames(sour))
sour[name_ind]
metaphorlity = metaphorlity[name_ind]
sentility = sentility[name_ind]
sour = sour[name_ind]
t(metaphorlity)
class(t(metaphorlity))
colSums(t(metaphorlity))
rowSums(t(metaphorlity))
apply(t(metaphorlity), 1, mean)
metaphorlity = apply(t(metaphorlity), 1, mean)
sentility = apply(t(sentility), 1, mean)
sour = apply(t(sour), 1, mean)
sour
df_test = cbind(metaphorlity, sentility)
View(df_test)
df_test = cbind(metaphorlity, sentility, sour)
mean_words
cv_words
survivor = sapply(1:length(mean_words), function(i) ifelse(mean_words>=1 & cv_words<=1, 1, 0))
survivor
survivor = sapply(1:length(mean_words), function(i) ifelse((mean_words>=1 & cv_words<=1), 1, 0))
survivor
survivor = sapply(ifelse((mean_words>=1 & cv_words<=1), 1, 0))
survivor = ifelse((mean_words>=1 & cv_words<=1), 1, 0)
survivor
df_test$survivor = survivor
df_test
df_test = cbind(metaphorlity, sentility, sour)
df_test
df_test = as.data.frame(df_test)
View(df_test)
df_test$survivor = survivor
tapply(df_test$metaphorlity, df_test$survivor, mean)
tapply(df_test$sentility, df_test$survivor, mean)
tapply(df_test$sour, df_test$survivor, mean)
colnames(df)[survivor==1]
colnames(df)[survivor==0]
cv_words
mean_words
survivor
colnames(df)[2:32][survivor==0]
colnames(df)[2:32][survivor==1]
colnames(df)[2:32][survivor==0]
colnames(df)[2:32][survivor==1]
colnames(df)[2:32][survivor==0]
library(caTools)
split = sample.split(df_test, SplitRatio = 0.8)
library(caTools)
set.seed(88)
split = sample.split(df_test, SplitRatio = 0.8)
words_train = subset(df_test, split = TRUE)
words_test = subset(df_test, split = FALSE)
words_train = subset(df_test, split = TRUE)
words_test = subset(df_test, split = FALSE)
View(df_test)
words_logistic = glm(survivor ~ ., data=df_test, family=binomial)
library(caTools)
set.seed(88)
split = sample.split(df_test, SplitRatio = 0.8)
words_train = subset(df_test, split = TRUE)
words_test = subset(df_test, split = FALSE)
words_logistic = glm(survivor ~ ., data=words_train, family=binomial)
summary(words_logistic)
predictTrain = predict(QualityLog, type="response")
predictTrain = predict(words_logistic, type="response")
predictTrain
table(words_train$survivor, predictTrain > 0.5)
table(words_train$survivor, predictTrain > 0.8)
table(words_train$survivor, predictTrain > 0.7)
table(words_train$survivor, predictTrain > 0.9)
library(rpart)
library(rpart.plot)
words_tree = rpart(survivor ~ ., data = words_train, method = "class")
prp(words_tree)
words_lm = lm(survivor ~ ., data = words_train)
summary(words_lm)
df_test$metaphorlity[df_test$survivor == 1]
tapply(df_test$metaphorlity, df_test$survivor, mean)
mean(df_test$metaphorlity[df_test$survivor == 1])
t.test(df_test$metaphorlity[df_test$survivor == 1])
t.test(df_test$metaphorlity[df_test$survivor == 0])
a = df_test$metaphorlity[df_test$survivor == 1]
b = df_test$metaphorlity[df_test$survivor == 0]
var.test(a,b)
t.test(a,b, var.equal=TRUE, paired=FALSE)
var.test(df_test$metaphorlity[df_test$survivor == 1],df_test$metaphorlity[df_test$survivor == 0])
t.test(df_test$metaphorlity[df_test$survivor == 1],df_test$metaphorlity[df_test$survivor == 0], var.equal=TRUE, paired=FALSE)
var.test(df_test$sentility[df_test$survivor == 1],df_test$sentility[df_test$survivor == 0])
t.test(df_test$sentility[df_test$survivor == 1],df_test$sentility[df_test$survivor == 0], var.equal=TRUE, paired=FALSE)
var.test(df_test$sentility[df_test$survivor == 1],df_test$sentility[df_test$survivor == 0])
t.test(df_test$sentility[df_test$survivor == 1],df_test$sentility[df_test$survivor == 0], var.equal=TRUE, paired=FALSE)
var.test(df_test$sour[df_test$survivor == 1],df_test$sour[df_test$survivor == 0])
t.test(df_test$sour[df_test$survivor == 1],df_test$sour[df_test$survivor == 0], var.equal=TRUE, paired=FALSE)
