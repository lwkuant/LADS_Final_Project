outcome <- 'churn'
pos <- '1'
useForCal <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0
dCal <- subset(dTrainAll,useForCal)
dTrain <- subset(dTrainAll,!useForCal)
# Using categorical features
table218 <- table(
Var218=dTrain[,'Var218'],
churn=dTrain[,outcome],
useNA='ifany')
print(table218)
print(table218[,2]/(table218[,1]+table218[,2]))
# Function to build single-variable models for categorical variables
mkPredC <- function(outCol,varCol,appCol) {
pPos <- sum(outCol==pos)/length(outCol)
naTab <- table(as.factor(outCol[is.na(varCol)]))
pPosWna <- (naTab/sum(naTab))[pos]
vTab <- table(as.factor(outCol),varCol)
pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3)
pred <- pPosWv[appCol]
pred[is.na(appCol)] <- pPosWna
pred[is.na(pred)] <- pPos
pred
}
for(v in catVars) {
pi <- paste('pred',v,sep='')
dTrain[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dTrain[,v])
dCal[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dCal[,v])
dTest[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dTest[,v])
}
# Scoring categorical variables by AUC
library('ROCR')
calcAUC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'auc')
as.numeric(perf@y.values)
}
for(v in catVars) {
pi <- paste('pred',v,sep='')
aucTrain <- calcAUC(dTrain[,pi],dTrain[,outcome])
if(aucTrain>=0.8) {
aucCal <- calcAUC(dCal[,pi],dCal[,outcome])
print(sprintf("%s, trainAUC: %4.3f calibrationAUC: %4.3f",
pi,aucTrain,aucCal))
}
}
# Scoring numeric variables by AUC
mkPredN <- function(outCol,varCol,appCol) {
cuts <- unique(as.numeric(quantile(varCol,
probs=seq(0, 1, 0.1),na.rm=T)))
varC <- cut(varCol,cuts)
appC <- cut(appCol,cuts)
mkPredC(outCol,varC,appC)
}
for(v in numericVars) {
pi <- paste('pred',v,sep='')
dTrain[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dTrain[,v])
dTest[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dTest[,v])
dCal[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dCal[,v])
aucTrain <- calcAUC(dTrain[,pi],dTrain[,outcome])
if(aucTrain>=0.55) {
aucCal <- calcAUC(dCal[,pi],dCal[,outcome])
print(sprintf("%s, trainAUC: %4.3f calibrationAUC: %4.3f",
pi,aucTrain,aucCal))
}
}
library(ggplot2)
ggplot(data=dCal) +
geom_density(aes(x=predVar126,color=as.factor(churn)))
# Running a repeated cross-validation experiment
var <- 'Var217'
aucs <- rep(0,100)
for(rep in 1:length(aucs)) {
useForCalRep <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0
predRep <- mkPredC(dTrainAll[!useForCalRep,outcome],
dTrainAll[!useForCalRep,var],
dTrainAll[useForCalRep,var])
aucs[rep] <- calcAUC(predRep,dTrainAll[useForCalRep,outcome])
}
mean(aucs)
sd(aucs)
# Empirically cross-validating performance
fCross <- function() {
useForCalRep <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0
predRep <- mkPredC(dTrainAll[!useForCalRep,outcome],
dTrainAll[!useForCalRep,var],dTrainAll[useForCalRep,var])
calcAUC(predRep,dTrainAll[useForCalRep,outcome])
}
aucs <- replicate(100,fCross())
aucs
## Basic variable selection
logLikelyhood <-
function(outCol,predCol) {
sum(ifelse(outCol==pos,log(predCol),log(1-predCol)))
}
selVars <- c()
minStep <- 5
baseRateCheck <- logLikelyhood(dCal[,outcome],
sum(dCal[,outcome]==pos)/length(dCal[,outcome]))
for(v in catVars) {
pi <- paste('pred',v,sep='')
liCheck <- 2*((logLikelyhood(dCal[,outcome],dCal[,pi]) -
baseRateCheck))
if(liCheck>minStep) {
print(sprintf("%s, calibrationScore: %g",
pi,liCheck))
selVars <- c(selVars,pi)
}
}
for(v in numericVars) {
pi <- paste('pred',v,sep='')
liCheck <- 2*((logLikelyhood(dCal[,outcome],dCal[,pi]) -
baseRateCheck) - 1)
if(liCheck>=minStep) {
print(sprintf("%s, calibrationScore: %g",
pi,liCheck))
selVars <- c(selVars,pi)
}
}
# Building a bad decision tree
library('rpart')
fV <- paste(outcome,'>0 ~ ',
paste(c(catVars,numericVars),collapse=' + '),sep='')
tmodel <- rpart(fV,data=dTrain)
print(calcAUC(predict(tmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(tmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(tmodel,newdata=dCal),dCal[,outcome]))
# Building another bad decision tree
tVars <- paste('pred',c(catVars,numericVars),sep='')
fV2 <- paste(outcome,'>0 ~ ',paste(tVars,collapse=' + '),sep='')
tmodel <- rpart(fV2,data=dTrain)
print(calcAUC(predict(tmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(tmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(tmodel,newdata=dCal),dCal[,outcome]))
# Building yet another bad decision tree
tmodel <- rpart(fV2,data=dTrain,
control=rpart.control(cp=0.001,minsplit=1000,
minbucket=1000,maxdepth=5)
)
print(calcAUC(predict(tmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(tmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(tmodel,newdata=dCal),dCal[,outcome]))
# Building a better decision tree
f <- paste(outcome,'>0 ~ ',paste(selVars,collapse=' + '),sep='')
tmodel <- rpart(f,data=dTrain,
control=rpart.control(cp=0.001,minsplit=1000,
minbucket=1000,maxdepth=5)
)
print(calcAUC(predict(tmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(tmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(tmodel,newdata=dCal),dCal[,outcome]))
print(tmodel)
tmodel
par(cex=0.7)
plot(tmodel)
text(tmodel)
print(plotROC(knnPred(dTest[,selVars]),dTest[,outcome]))
plotROC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'tpr','fpr')
pf <- data.frame(
FalsePositiveRate=perf@x.values[[1]],
TruePositiveRate=perf@y.values[[1]])
ggplot() +
geom_line(data=pf,aes(x=FalsePositiveRate,y=TruePositiveRate)) +
geom_line(aes(x=c(0,1),y=c(0,1)))
}
print(plotROC(knnPred(dTest[,selVars]),dTest[,outcome]))
library('class')
nK <- 200
knnTrain <- dTrain[,selVars]
knnCl <- dTrain[,outcome]==pos
knnPred <- function(df) {
knnDecision <- knn(knnTrain,df,knnCl,k=nK,prob=T)
ifelse(knnDecision==TRUE,
attributes(knnDecision)$prob,
1-(attributes(knnDecision)$prob))
}
print(calcAUC(knnPred(dTrain[,selVars]),dTrain[,outcome]))
print(calcAUC(knnPred(dCal[,selVars]),dCal[,outcome]))
print(calcAUC(knnPred(dTest[,selVars]),dTest[,outcome]))
dCal$kpred <- knnPred(dCal[,selVars])
ggplot(data=dCal) +
geom_density(aes(x=kpred,
color=as.factor(churn),linetype=as.factor(churn)))
plotROC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'tpr','fpr')
pf <- data.frame(
FalsePositiveRate=perf@x.values[[1]],
TruePositiveRate=perf@y.values[[1]])
ggplot() +
geom_line(data=pf,aes(x=FalsePositiveRate,y=TruePositiveRate)) +
geom_line(aes(x=c(0,1),y=c(0,1)))
}
print(plotROC(knnPred(dTest[,selVars]),dTest[,outcome]))
gmodel <- glm(as.formula(f),data=dTrain,family=binomial(link='logit'))
print(calcAUC(predict(gmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(gmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(gmodel,newdata=dCal),dCal[,outcome]))
gmodel <- glm(as.formula(f),data=dTrain,family=binomial(link='logit'))
print(calcAUC(predict(gmodel,newdata=dTrain),dTrain[,outcome]))
print(calcAUC(predict(gmodel,newdata=dTest),dTest[,outcome]))
print(calcAUC(predict(gmodel,newdata=dCal),dCal[,outcome]))
# VIDEO 4
# Read in data
wine = read.csv("D:/Kile/Coding/R programming/MITx 15.071x The Analytics Edge/Unit 2/wine.csv")
str(wine)
summary(wine)
# Linear Regression (one variable)
model1 = lm(Price ~ AGST, data=wine)
summary(model1)
# Sum of Squared Errors
model1$residuals
SSE = sum(model1$residuals^2)
SSE
# Linear Regression (two variables)
model2 = lm(Price ~ AGST + HarvestRain, data=wine)
summary(model2)
# Sum of Squared Errors
SSE = sum(model2$residuals^2)
SSE
# Linear Regression (all variables)
model3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
# Sum of Squared Errors
SSE = sum(model3$residuals^2)
SSE
# VIDEO 5
# Remove FrancePop
model4 = lm(Price ~ AGST + HarvestRain + WinterRain + Age, data=wine)
summary(model4)
model3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
# Sum of Squared Errors
SSE = sum(model3$residuals^2)
SSE
# VIDEO 5
# Remove FrancePop
model4 = lm(Price ~ AGST + HarvestRain + WinterRain + Age, data=wine)
summary(model4)
# VIDEO 6
# Correlations
cor(wine$WinterRain, wine$Price)
cor(wine$Age, wine$FrancePop)
cor(wine)
# Remove Age and FrancePop
model5 = lm(Price ~ AGST + HarvestRain + WinterRain, data=wine)
summary(model5)
# VIDEO 7
# Read in test set
wineTest = read.csv("D:/Kile/Coding/R programming/MITx 15.071x The Analytics Edge/Unit 2/wine_test.csv")
str(wineTest)
# Make test set predictions
predictTest = predict(model4, newdata=wineTest)
predictTest
# Compute R-squared
SSE = sum((wineTest$Price - predictTest)^2)
SST = sum((wineTest$Price - mean(wine$Price))^2)
1 - SSE/SST
# Unit 6 - Introduction to Clustering
# Video 6
# After following the steps in the video, load the data into R
movies = read.table("D:/Kile/Coding/R programming/MITx 15.071x The Analytics Edge/Unit 6/movieLens.txt", header=FALSE, sep="|",quote="\"")
View(movies)
?re.table
?read.table
str(movies)
# Add column names
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
str(movies)
# Remove unnecessary variables
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
# Remove duplicates
movies = unique(movies)
# Take a look at our data again:
str(movies)
# Video 7
# Compute distances
distances = dist(movies[2:20], method = "euclidean")
# Hierarchical clustering
clusterMovies = hclust(distances, method = "ward.D")
# Plot the dendrogram
plot(clusterMovies)
# Assign points to clusters
# k is equal to the number of clusters
clusterGroups = cutree(clusterMovies, k = 10)
#Now let's figure out what the clusters are like.
# Let's use the tapply function to compute the percentage of movies in each genre and cluster
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
clusterGroups
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
# We can repeat this for each genre. If you do, you get the results in ClusterMeans.ods
# Find which cluster Men in Black is in.
subset(movies, Title=="Men in Black (1997)")
clusterGroups[257]
# Create a new data set with just the movies from cluster 2
cluster2 = subset(movies, clusterGroups==2)
# Look at the first 10 titles in this cluster:
cluster2$Title[1:10]
clusterGroups[257]
# Create a new data set with just the movies from cluster 2
cluster2 = subset(movies, clusterGroups==2)
# Look at the first 10 titles in this cluster:
cluster2$Title[1:10]
View(baseball)
library(jiebaR)
a = "我是，好孩子，YA哈哈! no~ 笨笨!"
a
cutter = worker()
word = cutter[a]
word
class(word)
table(word)
paste(word[1], word[2])
paste(word[1], word[2],sep = "")
b = c("我", "是", "哈哈")
b
for(i in 1:length(word)){}
TRUE & TRUE
word_new = c()
for(i in 1:length(word)){
if(word[i] %in% b & word[i+1] %in% b){
word_new = c(word_new, paste(word[i], word[i+1], sep = ""))
}
}
word_new
seq(1,10,0.5)
length(seq(1,10,0.5))
length(seq(1,2,0.5))
x = scan()
x
chickwts
feeds = table(chickwts$feed)
feeds
barplot(feeds[order(feeds, decreasing = T)])
rep(c(1,2), each = 10)
summary(cars)
summary(iris)
str(cars)
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
list.dirs()
list.dirs()
list.files()
length(list.files())
df = read.csv(list.files()[1], stringsAsFactors = F)
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F)
View(df)
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F)
df = rbind(df, df_temp[2])
}
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
df = rbind(df, df_temp[,2])
}
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
df = cbind(df, df_temp[,2])
}
df_temp[,2]
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
View(df)
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
df = cbind(df, df_temp[,2])
}
View(df_temp)
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
df1 = read.csv(list.files()[2], stringsAsFactors = F, header = F)
View(df1)
?merge
View(df)
df1 = read.csv(list.files()[2], stringsAsFactors = F, header = F)
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
df1 = read.csv(list.files()[2], stringsAsFactors = F, header = F)
df2 = merge(df, df1, by = "V1")
View(df2)
df2 = merge(df, df1, by.x = "V1")
df2 = merge(df, df1, by.x = "V1", all.x = T)
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
df1 = read.csv(list.files()[2], stringsAsFactors = F, header = F)
df2 = merge(df, df1, all.x = T)
View(df2)
df2 = cbind(df, df1, all.x = T)
df2 = merge(df, df1, all = T)
View(df2)
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
read.csv
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
df1 = read.csv(list.files()[2], stringsAsFactors = F, header = F)
colnames(df1)[2] = "V3"
df2 = merge(df, df1, all.x = T)
View(df2)
list.files()
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-4)
df = merge(df,df_temp, all.x = T)
}
View(df)
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp, all.x = T)
}
View(df)
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("Date", substr(list.files()[1],1,nchar(list.files()[1])-12))
View(df)
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp, all.x = T)
}
mean_words = sapply(2:(ncol(df)), function(i) mean(df[,i], na.rm = T))
df[,1]
df[,2]
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("Date", substr(list.files()[1],1,nchar(list.files()[1])-12))
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp, all.x = T)
}
View(df)
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
View(df)
colnames(df) = c("Date", substr(list.files()[1],1,nchar(list.files()[1])-12))
df2
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
View(df)
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("Date", substr(list.files()[1],1,nchar(list.files()[1])-12))
for(i in 2:3){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp, all.x = T)
}
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("Date", substr(list.files()[1],1,nchar(list.files()[1])-12))
for(i in 2){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp, all.x = T)
}
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("Date", substr(list.files()[1],1,nchar(list.files()[1])-12))
for(i in 2){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp,by = "V1", all.x = T)
}
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("Date", substr(list.files()[1],1,nchar(list.files()[1])-12))
View(df)
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("V1", substr(list.files()[1],1,nchar(list.files()[1])-12))
View(df)
for(i in 2){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp,by = "V1", all.x = T)
}
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("V1", substr(list.files()[1],1,nchar(list.files()[1])-12))
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp,by = "V1", all.x = T)
}
mean_words = sapply(2:(ncol(df)), function(i) mean(df[,i], na.rm = T))
mean_words
is.na(df)
df[is.na(df)]
df[is.na(df)] = 0
View(df)
setwd("D:/Kile/語言分析與資料科學/Final/word_data/2015_time_series")
df = read.csv(list.files()[1], stringsAsFactors = F, header = F)
colnames(df) = c("V1", substr(list.files()[1],1,nchar(list.files()[1])-12))
for(i in 2:length(list.files())){
df_temp = read.csv(list.files()[i], stringsAsFactors = F, header = F)
colnames(df_temp)[2] = substr(list.files()[i],1,nchar(list.files()[i])-12)
df = merge(df,df_temp,by = "V1", all.x = T)
}
df[is.na(df)] = 0
mean_words = sapply(2:(ncol(df)), function(i) mean(df[,i]))
mean_words
stdev_words = sapply(2:(ncol(df)), function(i) sd(df[,i]))
stdev_words
